(ns forma.classify.logistic
  (:use [forma.utils]
        [forma.schema :only (unpack-neighbor-val)]
        [clojure.math.numeric-tower :only (abs)]
        [forma.matrix.utils]
        [cascalog.api])
  (:require [incanter.core :as i])
  (:import [org.jblas FloatMatrix MatrixFunctions Solve DoubleMatrix]))

;; TODO: correct for error induced by ridge

;; Namespace Conventions: Each observation is assigned a binary
;; `label` which indicates deforestation during the training period.
;; These labels are collected for a group of pixels into `label-seq`.
;; Each pixel also has a sequence of features, or `feature-seq`.  The
;; pixel is identified by the order its attributes appear in the
;; feature and label sequence.  That is, it is vital that the labels
;; and feature sequences are consistently positioned in the label and
;; feature collections.

(defn logistic-fn
  "returns the value of the logistic function, given input `x`"
  [x]
  (let [exp-x (Math/exp x)]
    (/ exp-x (inc exp-x))))

(defn to-double-matrix
  "returns a DoubleMatrix instance for use with jBLAS functions"
  [mat]
  (DoubleMatrix.
   (into-array (map double-array mat))))

(defn logistic-prob
  "returns the probability of a binary outcome given a parameter
  vector `beta-seq` and a feature vector for a given observation"
  [beta-seq feature-seq]
  (logistic-fn (dot-product beta-seq feature-seq)))

(defn log-likelihood
  "returns the log likelihood of a given pixel, conditional on its
  label (0-1) and the probability of label equal to 1."
  [beta-seq label feature-seq]
  (let [prob (logistic-prob beta-seq feature-seq)]
    (+ (* label (Math/log prob))
       (* (- 1 label) (Math/log (- 1 prob))))))

(defn total-log-likelihood
  "returns the total log likelihood for a group of pixels; input
  labels and features for the group of pixels, aligned correctly so
  that the first label and feature correspond to the first pixel."
  [beta-seq label-seq feature-mat]
  (reduce + (map (partial log-likelihood beta-seq)
                 label-seq
                 feature-mat)))

(defn probability-calc
  "returns a vector of probabilities for each observation"
  [beta-seq feature-mat]
  (map (partial logistic-prob beta-seq)
       feature-mat))

(defn score-seq
  "returns the scores for each parameter"
  [beta-seq label-seq feature-mat]
  (let [prob-seq (probability-calc beta-seq feature-mat)
        features (to-double-matrix feature-mat)]
    (.mmul (.transpose features)
           (DoubleMatrix.
            (double-array
             (map - label-seq prob-seq))))))

(defn info-matrix
  "returns the square information matrix for the logistic probability
  function; the dimension is given by the number of features"
  [beta-seq feature-mat]
  (let [mult-func (fn [x] (* x (- 1 x)))
        prob-seq  (->> (probability-calc beta-seq feature-mat)
                       (map mult-func))
        scale-feat (multiply-rows
                    prob-seq
                    (transpose feature-mat))]
    (.mmul (to-double-matrix scale-feat)
           (to-double-matrix feature-mat))))

(defn beta-update
  "returns a vector of updates for the parameter vector; the
  ridge-constant is a very small scalar, used to ensure that the
  inverted information matrix is non-singular."
  [beta-seq label-seq feature-mat rdg-cons]
  (let [num-features (count beta-seq)
        info-adj (.addi
                  (info-matrix beta-seq feature-mat)
                  (.muli (DoubleMatrix/eye (int num-features))
                         (float rdg-cons)))]
    (vec (.toArray
          (.mmul (Solve/solve
                  info-adj
                  (DoubleMatrix/eye (int num-features)))
                 (score-seq beta-seq label-seq feature-mat))))))

(defn logistic-beta-vector
  "return the estimated parameter vector; which is used, in turn, to
  calculate the estimated probability of the binary label"
  [label-seq feature-mat rdg-cons converge-threshold max-iter]
  (let [beta-init (repeat (count (first feature-mat)) 0)]
    (loop [beta beta-init
           iter max-iter
           beta-diff 100]
      (if (or (zero? iter)
              (< beta-diff converge-threshold))
        beta
        (let [update (beta-update beta label-seq feature-mat rdg-cons)
              beta-new (map + beta update)
              diff (reduce + (map (comp abs -) beta beta-new))]
          (recur
           beta-new
           (dec iter)
           diff))))))

(defn estimated-probabilities
  "returns the set of probabilities, after applying the parameter
  values estimated over the training data; both `label-seq` and
  `traning-features` reflect data over the training period and
  `updated-features` reflect data through some interval, which could
  be the end of the training period (for internal validataion) but is
  most likely some interval thereafter"
  [label-seq training-features updated-features]
  (let [new-beta (logistic-beta-vector
                  label-seq
                  training-features
                  1e-8
                  1e-6
                  250)]
    (probability-calc new-beta updated-features)))

(defn unpack-neighbors
  "Returns a vector containing the fields of a forma-neighbor-value."
  [neighbor-val]
  (map (partial get neighbor-val)
       [:fire-value :neighbor-count
        :avg-short-drop :min-short-drop
        :avg-long-drop :min-long-drop
        :avg-t-stat :min-t-stat]))

(defn unpack-fire [fire]
  (map (partial get fire)
       [:temp-330 :conf-50 :both-preds :count]))

(defn unpack-feature-vec [val neighbor-val]
  (let [[fire short _ long t-stat] val
        fire-seq (unpack-fire fire)
        [fire-neighbors & more] (unpack-neighbors neighbor-val)
        fire-neighbor (unpack-fire fire-neighbors)]
    (into [] (concat fire-seq [short long t-stat] fire-neighbor more))))

(defn make-binary
  [x]
  (if (zero? x) 0 1))

(defbufferop [logistic-beta-wrap [r c m]]
  "returns a vector of parameter coefficients.  note that this is
  where the intercept is added (to the front of each stacked vector in
  the feature matrix

  TODO: The intercept is included in the feature vector for now, as a
  kludge when we removed the hansen statistic.  When we include the
  hansen stat, we will have to replace the feature-mat binding below
  with a line that tacks on a 1 to each feature vector.
  "
  [tuples]
  (let [label-seq    (map (comp make-binary first) tuples) 
        val-mat      (map second tuples) 
        neighbor-mat (map last tuples)
        feature-mat  (map unpack-feature-vec val-mat neighbor-mat)]
    [[(logistic-beta-vector label-seq feature-mat r c m)]]))

(defn logistic-prob-wrap
  [beta-vec val neighbor-val]
  (logistic-prob beta-vec (unpack-feature-vec val neighbor-val)))

(defbufferop mk-timeseries
  [tuples]
  [[(map second (sort-by first tuples))]])

(def beta-map
  {:40159 [0.0 0.0 0.0 0.0 0.06325956015102006 -0.16519407866340646 -0.5348362045894003 0.0 0.0 0.0 0.0 -0.686224132946902 -0.018079615166628565 0.026912345439857023 0.08109012937829134 0.007560497631149489 -0.3501106177352919 -0.8871233045997788]
   :10123 [0.0 0.0 0.0 0.0 0.03392775084874883 -0.0750720595011365 -0.435462142731104 0.0 0.0 0.0 0.0 -0.4759610994062899 0.013976051770553782 -0.003617608729219433 -0.019170331084499267 -0.038044679349151705 -0.0679082212239463 -0.08132869405242005]
   :10121 [0.0 0.0 0.0 0.0 0.02677226018430434 0.1919661493624154 -1.2146362403571676 0.0 0.0 0.0 0.0 -0.3509415866016143 0.011988195816147872 0.005360890127489654 0.3242874064222846 -0.6344923889615051 -1.444573944653287 2.4684880401761844]
   :10120 [0.0 0.0 0.0 0.0 0.052280020671562795 -0.08250235778829386 -0.03422867711986093 0.0 0.0 0.0 0.0 -0.8656938912543279 0.03891384326097658 -0.02906099586424071 -0.01898255441647896 -0.14655765678848592 -0.11249509663222074 0.1590290124185424]
   :40167 [0.0 0.0 0.0 0.0 0.082413706765644 -0.5447733554497433 0.09217365420072017 0.0 0.0 0.0 0.0 -0.5503880024052501 0.017844705169004917 0.01493297066119657 0.448195543564665 -0.48873977991742446 -1.1067013204064962 0.8329238046724056]
   :10116 [0.0 0.0 0.0 0.0 0.10923774774711319 0.22060799483261484 -1.4247198801363865 0.0 0.0 0.0 0.0 -0.571676711462897 -0.005736867272562124 0.01479714323550888 -0.530553369169624 -0.009741214374878236 1.4322942244797456 0.3555988809045467]
   :40160 [0.0 0.0 0.0 0.0 0.007879511884894224 -0.06437217183847117 -0.23516072213103337 0.0 0.0 0.0 0.0 -0.5470938721638846 0.0025144583692929072 -0.018438596530528897 -0.1009595658763844 -0.16543146759287972 0.3161222552535783 0.11713069779475117]
   :10107 [0.0 0.0 0.0 0.0 0.08636367599774337 -0.3097245677829795 0.32190405307090836 0.0 0.0 0.0 0.0 -0.7132674841514093 -0.003720770743729832 0.014370708530342094 0.18587558067406712 -0.4551033752386946 -0.8494345937776441 1.3332542829351566]
   :40157 [0.0 0.0 0.0 0.0 0.0175097539346876 -0.5110089115993631 1.2212204360371268 0.0 0.0 0.0 0.0 -0.4149202699487292 0.013493649533632752 -0.017542694291041203 -0.20761081058966388 -0.0021420597597087486 0.672067399798538 -0.5735554049757167]
   :10104 [0.0 0.0 0.0 0.0 0.03450534145752235 -0.2617492468546382 0.09209780374205769 0.0 0.0 0.0 0.0 -0.709856205386332 -0.0028990809106214945 -0.00313440156854547 -0.38192859577253024 0.48766898793833746 0.6604499698393929 -1.3621697218952067]
   :40104 [0.0 0.0 0.0 0.0 0.014402612884093606 -0.16602452994725758 0.14198127076704734 0.0 0.0 0.0 0.0 -0.6359830270954929 0.012986125187551009 -0.023267572094263018 -0.15388093814149562 -0.22694256133504262 0.38146377930706404 0.37367536630091985]
   :40304 [0.0 0.0 0.0 0.0 0.09120807979700796 -0.5158913374210704 -0.11060879946885735 0.0 0.0 0.0 0.0 -1.0790886279887615 0.030234497602954605 -0.08395600454475587 -0.7534900798437293 2.0874445194314464 3.6120385564683386 -8.298432689952849]
   :41404 [0.0 0.0 0.0 0.0 0.013108672191445302 -0.03756819904754859 -0.532101536250742 0.0 0.0 0.0 0.0 -0.575268723681948 0.012792687667906066 -0.008339156430061254 -0.01127281961500675 0.08189829541208585 0.0077441959765922426 -0.28929381023681855]
   :10101 [0.0 0.0 0.0 0.0 0.10894308099002774 -0.8727400335859836 2.4589987388974657 0.0 0.0 0.0 0.0 -0.21046874055007786 0.04863167410637211 -0.020825273215806475 0.08714670162565553 -0.07926854947416981 -0.5864675715216993 0.1612457107126011]
   :11401 [0.0 0.0 0.0 0.0 0.04028451218010588 -0.08112818418325904 -0.28033122755022966 0.0 0.0 0.0 0.0 -0.29827294562132795 -0.015464604627767815 0.047860754458635905 0.03117040933577286 -0.3955897669901828 -0.16657698888718564 1.267198417480579]
   :41001 [0.0 0.0 0.0 0.0 -0.014411971481690601 -0.6048034209994457 1.2404903849113669 0.0 0.0 0.0 0.0 -0.8973688838152453 0.009917092235814797 0.02402229958324418 -0.31608843025850225 -0.32190302600171783 1.0263153103552947 -0.23930496430687528]
   :10128 [0.0 0.0 0.0 0.0 0.02375087181779681 0.0470820048640427 -0.5416646949723481 0.0 0.0 0.0 0.0 -0.6881928329305697 0.01032434072572802 1.1521397286254223E-4 0.03003436451241647 -0.18259924639913705 -0.03240684292846756 0]	
   :40128 [0.0 0.0 0.0 0.0 0.08721258037488301 0.3184024004675204 -2.1089528838579192 0.0 0.0 0.0 0.0 -0.5854625326508828 0.008085051581233876 -0.003817830657735756 0.3215548797402081 0.006487240129291144 -0.8866731498174856 -0.11452382177151627]
   :10127 [0.0 0.0 0.0 0.0 0.028440579964402954 0.12481299751944246 -0.6704304590249431 0.0 0.0 0.0 0.0 -1.0686199083457597 0.014729535335445892 -0.01268106664403002 0.13091614104691823 0.0888983248289259 -1.1830072584579998 -0.007400279655881172]
   :40127 [0.0 0.0 0.0 0.0 0.03458854348578056 0.19114646110798666 -0.6575459008440561 0.0 0.0 0.0 0.0 -0.3676132005843386 0.001274052771285028 0.04002823214225964 -0.3184338428371418 0.0016119007777181387 0.4798196357856604 0.1710135253486781]
   :10118 [0.0 0.0 0.0 0.0 0.022836063310224045 -0.026494371623404064 -0.271265324737098 0.0 0.0 0.0 0.0 -0.8432308091495382 0.014198942979559375 -0.011950729336358578 -0.320559430228316 0.3724592561991763 1.0550628801481428 -1.8524046192215642]
   :40168 [0.0 0.0 0.0 0.0 0.07357062776814331 -0.4402450264204722 0.44031215432117093 0.0 0.0 0.0 0.0 -0.39249669044885577 0.021756485380521436 -1.035226142443395E-4 0.08508389233061074 -0.4730358723877611 -0.4408439333972656 0]	
   :40114 [0.0 0.0 0.0 0.0 0.06347648464334063 -0.21598828595283506 0.34207665410309857 0.0 0.0 0.0 0.0 -0.16013001093230206 -1.795863276696411E-4 0.01686897376384659 0.021865133506453333 0.056046188188197676 -0.38335383570532316 -0]	
   :40164 [0.0 0.0 0.0 0.0 2.3956086453299377 2.6834036981585996 -11.632698920954061 0.0 0.0 0.0 0.0 0.29727911163587406 0.730459080808941 -0.33889757996228737 -1.5151200169403547 2.2496435142506543 1.7427129296469672 -1.3464433632033412]
   :40113 [0.0 0.0 0.0 0.0 0.09432910244602653 -0.6166238235554541 0.6301155677622806 0.0 0.0 0.0 0.0 -0.49465395007309754 0.008826308711333598 0.00151995580617714 0.0940721255033007 -0.39056772557053543 0.02848837200870915 0.45290608893554357]
   :40163 [0.0 0.0 0.0 0.0 0.02589160003576327 -0.1634103769181009 -0.25078565527010993 0.0 0.0 0.0 0.0 -0.37405760559639273 0.0017553347205862583 -7.458745443288981E-4 -0.20239157122108203 0.24889739705454197 0.807556610561391 -1]	
   :10112 [0.0 0.0 0.0 0.0 0.7682240977249429 0.5883382686522205 -5.816127140523921 0.0 0.0 0.0 0.0 -1.4338033902281646 0.08412321808066411 0.1743468020445252 -0.6727785440486591 -2.1113836425632617 2.0800942280939054 10.278440803969541]
   :40112 [0.0 0.0 0.0 0.0 0.07478898911206434 -0.4462535823456842 0.23674368551045022 0.0 0.0 0.0 0.0 -0.3715242834758344 0.027968153661946903 0.01310304711444054 0.0736855558514591 -0.5531929823303042 -0.4477737654905845 1.1857112524628906]
   :10111 [0.0 0.0 0.0 0.0 0.0030787971034627144 -0.03233820407955067 -0.2635412169259933 0.0 0.0 0.0 0.0 -0.9496295992748299 0.014528730837835981 -0.02127650474047189 -0.0925641850847324 0.5328675533165826 0.022969314351042273 -1.8933716464439898]
   :40161 [0.0 0.0 0.0 0.0 0.026574884553712708 -0.47886261861683355 0.7806757098540203 0.0 0.0 0.0 0.0 -0.6738044079293856 0.012934866831723289 -0.019998635876231175 -0.19531518792148472 -0.09877900963551002 0.41253524716605217 -0.023379713855903893]
   :10124 [0.0 0.0 0.0 0.0 0.05607359012193863 0.11073197175693862 -1.120512707714171 0.0 0.0 0.0 0.0 -0.7208595239056542 0.028043806821461358 -0.021423660379943123 0.05381316835684429 -0.20095618912285315 -0.20613726357234324 -0.1636945207323316]
   :10106 [0.0 0.0 0.0 0.0 0.01642712136570199 -0.06794122003750247 -0.35429070295404264 0.0 0.0 0.0 0.0 -0.6629374392575458 0.0022587201016010645 0.004296922743621863 0.07980733759509388 -0.04550664605249228 -0.3401421162957198 -0.4114120059567254]
   :40156 [0.0 0.0 0.0 0.0 0.04261179461358588 0.005981778919758764 -1.1947873703606655 0.0 0.0 0.0 0.0 -0.4678278425510944 0.026158056132372 -0.007186909346480664 0.30391888840913994 -0.08794296100363186 -1.5440884748998198 0.8220149019492653]
   :40149 [0.0 0.0 0.0 0.0 0.036086519339814854 0.01003084321370003 0.04262813291728135 0.0 0.0 0.0 0.0 -0.22399587256967265 0.03986609442571241 0.09231363383148013 -0.2402334876569829 0.017845907123513197 0.04624615597312326 0.018808971782846767]
   :-9999 [0.0 0.0 0.0 0.0 0.06125025780471143 -0.14486714736221962 0.12941003521514907 0.0 0.0 0.0 0.0 -0.6900830337688446 0.00793132943732828 0.0023543434138861927 0.06617334353195989 -0.1376485317170391 -0.22116341063486972 0.07988658714082639]
   :40145 [0.0 0.0 0.0 0.0 -0.013214576617650879 -0.16722213274498682 -0.03654756762099797 0.0 0.0 0.0 0.0 -0.6596691441667684 0.00519088539935908 -0.013734285443183369 0.105589320726195 -0.050744955819452896 -0.4137498559730518 -0.12196355421706696]
   :40144 [0.0 0.0 0.0 0.0 -0.013109301843950897 -0.1932661558427634 -0.037310156477377406 0.0 0.0 0.0 0.0 -1.2597640669415786 0.030474839265170695 -0.03781240937338362 -0.186795487295565 0.34420307771452163 0.1588078599007017 -0.7380187385847832]
   :40143 [0.0 0.0 0.0 0.0 0.0875030310075549 -0.5459455708590712 0.6695644878543994 0.0 0.0 0.0 0.0 -0.4550290729852864 0.02649171156602579 0.014947081492296895 -0.4217233830096652 0.0026985095947234324 1.3638937533136826 -1.0211158005303933]
   :40133 [0.0 0.0 0.0 0.0 0.0092107779633259 -0.011457031259328556 -1.14531261607155 0.0 0.0 0.0 0.0 -0.9221141758102631 0.00311348852813217 0.00666942339234123 -0.1297580909731859 0.005263846893830406 1.2860200710774137 -0.8922301139889142]
   :40129 [0.0 0.0 0.0 0.0 0.07600601215177051 -0.0988953264366778 -0.6411172163380331 0.0 0.0 0.0 0.0 -0.5582100833470203 0.02040150260230132 -0.02164450739924 -0.037654639755694004 0.047628216543345155 -0.009246076216060953 -0.3075417164941555]
   :10125 [0.0 0.0 0.0 0.0 0.07678210291075997 -0.22275505748385774 0.36181054012676245 0.0 0.0 0.0 0.0 -0.5610429123350986 -0.04620485914010856 0.05307194689063476 -0.033185599563048306 -0.23892702268170996 0.30373889078795757 1.2200870785499567]})
